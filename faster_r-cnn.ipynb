{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-17T09:17:45.441954Z","iopub.status.busy":"2022-04-17T09:17:45.441669Z","iopub.status.idle":"2022-04-17T09:17:49.082579Z","shell.execute_reply":"2022-04-17T09:17:49.08184Z","shell.execute_reply.started":"2022-04-17T09:17:45.441925Z"},"trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-17T09:17:49.085911Z","iopub.status.busy":"2022-04-17T09:17:49.085681Z","iopub.status.idle":"2022-04-17T09:18:25.244767Z","shell.execute_reply":"2022-04-17T09:18:25.243768Z","shell.execute_reply.started":"2022-04-17T09:17:49.085885Z"},"trusted":true},"outputs":[],"source":["# !pip install albumentations==0.5.2\n","!pip install torchvision -U\n","!pip install torch -U\n","!pip install pycocotools\n","!pip install natsort"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-17T09:18:25.247019Z","iopub.status.busy":"2022-04-17T09:18:25.246751Z","iopub.status.idle":"2022-04-17T09:18:25.973063Z","shell.execute_reply":"2022-04-17T09:18:25.972174Z","shell.execute_reply.started":"2022-04-17T09:18:25.246983Z"},"trusted":true},"outputs":[],"source":["!git clone -b release/0.12 https://github.com/pytorch/vision.git"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-17T09:18:25.977805Z","iopub.status.busy":"2022-04-17T09:18:25.977525Z","iopub.status.idle":"2022-04-17T09:18:26.680123Z","shell.execute_reply":"2022-04-17T09:18:26.679212Z","shell.execute_reply.started":"2022-04-17T09:18:25.977774Z"},"trusted":true},"outputs":[],"source":["!cat ./vision/version.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-17T09:18:27.391363Z","iopub.status.busy":"2022-04-17T09:18:27.391126Z","iopub.status.idle":"2022-04-17T09:18:28.092632Z","shell.execute_reply":"2022-04-17T09:18:28.091669Z","shell.execute_reply.started":"2022-04-17T09:18:27.391337Z"},"trusted":true},"outputs":[],"source":["!mv ./vision/references/detection/*.py ."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-17T09:18:28.913378Z","iopub.status.busy":"2022-04-17T09:18:28.91293Z","iopub.status.idle":"2022-04-17T09:18:28.926447Z","shell.execute_reply":"2022-04-17T09:18:28.925582Z","shell.execute_reply.started":"2022-04-17T09:18:28.913338Z"},"id":"kxN229aUS--0","trusted":true},"outputs":[],"source":["# Import necessary packages.\n","import numpy as np\n","import pandas as pd\n","import torch\n","import os\n","import torch.nn as nn\n","import vision.torchvision as torchvision\n","# import torchvision.transforms as transforms\n","from PIL import Image, ImageChops\n","# \"ConcatDataset\" and \"Subset\" are possibly useful when doing semi-supervised learning.\n","from torch.utils.data import ConcatDataset, DataLoader, Subset, Dataset\n","from torchvision.datasets import DatasetFolder, VisionDataset\n","\n","# This is for the progress bar.\n","from tqdm.auto import tqdm\n","import random\n","\n","# For plotting learning curve\n","from torch.utils.tensorboard import SummaryWriter\n","\n","# from pytorch_grad_cam import GradCAM, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, EigenCAM, FullGrad\n","# from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n","# from pytorch_grad_cam.utils.image import show_cam_on_image\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns \n","%matplotlib inline\n","import cv2\n","import albumentations as A\n","import albumentations.pytorch\n","import xml.etree.ElementTree as ET\n","from natsort import os_sorted"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-17T09:18:29.664938Z","iopub.status.busy":"2022-04-17T09:18:29.664659Z","iopub.status.idle":"2022-04-17T09:18:29.995175Z","shell.execute_reply":"2022-04-17T09:18:29.994446Z","shell.execute_reply.started":"2022-04-17T09:18:29.664909Z"},"trusted":true},"outputs":[],"source":["dataset_path = \"../input/chest-xray-box\"\n","pneumonia_dir = dataset_path+\"/PNEUMONIA/JPEGFiles\"\n","normal_dir = dataset_path+\"/NORMAL/JPEGFiles\"\n","pneumonia_files = os_sorted(os.listdir(os.path.join(dataset_path, 'PNEUMONIA/JPEGFiles')))\n","normal_files = os_sorted(os.listdir(os.path.join(dataset_path, 'NORMAL/JPEGFiles')))"]},{"cell_type":"markdown","metadata":{},"source":["## Transformation"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-17T09:18:30.546722Z","iopub.status.busy":"2022-04-17T09:18:30.54624Z","iopub.status.idle":"2022-04-17T09:18:30.557233Z","shell.execute_reply":"2022-04-17T09:18:30.556395Z","shell.execute_reply.started":"2022-04-17T09:18:30.546672Z"},"trusted":true},"outputs":[],"source":["train_tfm = A.Compose([\n","            A.augmentations.geometric.resize.Resize(299,299),\n","            A.augmentations.geometric.transforms.Affine(translate_percent=(0.15, 0.2), rotate=20),\n","            # A.HorizontalFlip(p=0.5),\n","            A.pytorch.transforms.ToTensorV2(),\n","            ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']))\n","valid_tfm = A.Compose([\n","            A.augmentations.geometric.resize.Resize(299,299),\n","            # A.HorizontalFlip(p=0.5),\n","            A.pytorch.transforms.ToTensorV2(),\n","            ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']))\n","test_tfm = A.Compose([\n","            A.augmentations.geometric.resize.Resize(299,299),\n","            # A.HorizontalFlip(p=0.5),\n","            A.pytorch.transforms.ToTensorV2(),\n","            ])"]},{"cell_type":"markdown","metadata":{},"source":["# Training"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-17T09:18:30.582219Z","iopub.status.busy":"2022-04-17T09:18:30.58169Z","iopub.status.idle":"2022-04-17T09:18:30.605677Z","shell.execute_reply":"2022-04-17T09:18:30.604851Z","shell.execute_reply.started":"2022-04-17T09:18:30.582181Z"},"trusted":true},"outputs":[],"source":["import math\n","import sys\n","import time\n","\n","# import torch\n","# import torchvision.models.detection.mask_rcnn\n","import utils\n","from coco_eval import CocoEvaluator\n","from coco_utils import get_coco_api_from_dataset\n","\n","\n","def train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq, scaler=None):\n","    model.train()\n","    metric_logger = utils.MetricLogger(delimiter=\"  \")\n","    metric_logger.add_meter(\"lr\", utils.SmoothedValue(window_size=1, fmt=\"{value:.6f}\"))\n","    header = f\"Epoch: [{epoch}]\"\n","\n","    lr_scheduler = None\n","    if epoch == 0:\n","        warmup_factor = 1.0 / 1000\n","        warmup_iters = min(1000, len(data_loader) - 1)\n","\n","        lr_scheduler = torch.optim.lr_scheduler.LinearLR(\n","            optimizer, start_factor=warmup_factor, total_iters=warmup_iters\n","        )\n","\n","    for images, targets in metric_logger.log_every(data_loader, print_freq, header):\n","        images = list(image.to(device) for image in images)\n","        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n","        with torch.cuda.amp.autocast(enabled=scaler is not None):\n","            loss_dict = model(images, targets)\n","            losses = sum(loss for loss in loss_dict.values())\n","\n","        # reduce losses over all GPUs for logging purposes\n","        loss_dict_reduced = utils.reduce_dict(loss_dict)\n","        losses_reduced = sum(loss for loss in loss_dict_reduced.values())\n","\n","        loss_value = losses_reduced.item()\n","\n","        if not math.isfinite(loss_value):\n","            print(f\"Loss is {loss_value}, stopping training\")\n","            print(loss_dict_reduced)\n","            sys.exit(1)\n","\n","        optimizer.zero_grad()\n","        if scaler is not None:\n","            scaler.scale(losses).backward()\n","            scaler.step(optimizer)\n","            scaler.update()\n","        else:\n","            losses.backward()\n","            optimizer.step()\n","\n","        if lr_scheduler is not None:\n","            lr_scheduler.step()\n","\n","        metric_logger.update(loss=losses_reduced, **loss_dict_reduced)\n","        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n","\n","    return metric_logger\n","\n","\n","def _get_iou_types(model):\n","    model_without_ddp = model\n","    if isinstance(model, torch.nn.parallel.DistributedDataParallel):\n","        model_without_ddp = model.module\n","    iou_types = [\"bbox\"]\n","    if isinstance(model_without_ddp, torchvision.models.detection.MaskRCNN):\n","        iou_types.append(\"segm\")\n","    if isinstance(model_without_ddp, torchvision.models.detection.KeypointRCNN):\n","        iou_types.append(\"keypoints\")\n","    return iou_types\n","\n","\n","@torch.inference_mode()\n","def evaluate(model, data_loader, device):\n","    n_threads = torch.get_num_threads()\n","    # FIXME remove this and make paste_masks_in_image run on the GPU\n","    torch.set_num_threads(1)\n","    cpu_device = torch.device(\"cpu\")\n","    model.eval()\n","    metric_logger = utils.MetricLogger(delimiter=\"  \")\n","    header = \"Test:\"\n","\n","    coco = get_coco_api_from_dataset(data_loader.dataset)\n","    iou_types = _get_iou_types(model)\n","    coco_evaluator = CocoEvaluator(coco, iou_types)\n","\n","    for images, targets in metric_logger.log_every(data_loader, 100, header):\n","        images = list(img.to(device) for img in images)\n","\n","        if torch.cuda.is_available():\n","            torch.cuda.synchronize()\n","        model_time = time.time()\n","        outputs = model(images)\n","\n","        outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n","        model_time = time.time() - model_time\n","        # print(\"t\", targets)\n","        # print(\"o\", outputs)\n","        res = { targets[\"image_id\"].item(): output for output in outputs}\n","        # print(res)\n","        # res = {target[\"image_id\"].item(): output for target, output in zip(targets, outputs)}\n","        evaluator_time = time.time()\n","        coco_evaluator.update(res)\n","        evaluator_time = time.time() - evaluator_time\n","        metric_logger.update(model_time=model_time, evaluator_time=evaluator_time)\n","\n","    # gather the stats from all processes\n","    metric_logger.synchronize_between_processes()\n","    print(\"Averaged stats:\", metric_logger)\n","    coco_evaluator.synchronize_between_processes()\n","\n","    # accumulate predictions from all images\n","    coco_evaluator.accumulate()\n","    coco_evaluator.summarize()\n","    torch.set_num_threads(n_threads)\n","    return coco_evaluator"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-17T09:18:30.689228Z","iopub.status.busy":"2022-04-17T09:18:30.688779Z","iopub.status.idle":"2022-04-17T09:18:30.695468Z","shell.execute_reply":"2022-04-17T09:18:30.694767Z","shell.execute_reply.started":"2022-04-17T09:18:30.689192Z"},"id":"KQP3C-pDS--0","trusted":true},"outputs":[],"source":["def train_valid_test_split(folder, train_size, valid_size, test_size):\n","    # 0 = train, 1 = valid, 2 = test\n","    \n","    train_files=[]\n","    valid_files=[]\n","    test_files=[]\n","    \n","    for file in folder:\n","        if len(train_files) <= train_size:\n","            train_files.append(file)\n","        elif len(valid_files) <= (valid_size):\n","            valid_files.append(file)\n","        else:\n","            test_files.append(file)\n","  \n","    return train_files, valid_files, test_files\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-17T09:18:30.697216Z","iopub.status.busy":"2022-04-17T09:18:30.69675Z","iopub.status.idle":"2022-04-17T09:18:30.708387Z","shell.execute_reply":"2022-04-17T09:18:30.707734Z","shell.execute_reply.started":"2022-04-17T09:18:30.69718Z"},"id":"RBcxxQ-RS--1","trusted":true},"outputs":[],"source":["# p_train_files, p_valid_files, p_test_files = train_valid_test_split(pneumonia_files, 3673, 300, 300)\n","# n_train_files, n_valid_files, n_test_files = train_valid_test_split(normal_files, 1083, 250, 250)\n","p_train_files, p_valid_files, p_test_files = train_valid_test_split(pneumonia_files, 160, 40, 4073)\n","n_train_files, n_valid_files, n_test_files = train_valid_test_split(normal_files, 160, 40, 1383)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-17T09:18:30.757214Z","iopub.status.busy":"2022-04-17T09:18:30.756825Z","iopub.status.idle":"2022-04-17T09:18:30.773437Z","shell.execute_reply":"2022-04-17T09:18:30.772754Z","shell.execute_reply.started":"2022-04-17T09:18:30.757178Z"},"id":"wDk-gX-WS--2","trusted":true},"outputs":[],"source":["class ChestXRayDatasetTrain(Dataset):\n","\n","    def __init__(self, files, label, tfm=None):\n","        data_path = \"../input/chest-xray-box\"\n","        self.files = files\n","        self.transforms = tfm\n","        self.label = label\n","        if label == 1:\n","            data_path = data_path+\"/NORMAL\"\n","        else:\n","            data_path = data_path+\"/PNEUMONIA\"\n","        self.img_files = [os.path.join(data_path+\"/JPEGFiles\", x) for x in files]\n","        self.rectboxes = [os.path.join(data_path+\"/Rectbox\", x.replace(\".jpeg\",\".xml\")) for x in files]\n","    \n","    def __len__(self):\n","        return len(self.files)\n","  \n","    def __getitem__(self,idx):\n","        fname = self.img_files[idx]\n","        gray_img = cv2.imread(fname, cv2.IMREAD_GRAYSCALE)\n","        img = cv2.cvtColor(gray_img, cv2.COLOR_GRAY2BGR)\n","        img = img.astype(np.float32)\n","        img /= 255\n","        # im = torch.tensor(im)\n","        # img = self.transform(img)\n","        num_objs = 0\n","           \n","        xml = ET.parse(self.rectboxes[idx])\n","        root = xml.getroot()\n","        obj = root.find(\"object\")\n","        bndbox = obj.find(\"bndbox\")\n","        \n","        boxes = []\n","        xmin = int(bndbox[0].text)\n","        ymin = int(bndbox[1].text)\n","        xmax = int(bndbox[2].text)\n","        ymax = int(bndbox[3].text)\n","        boxes.append([xmin, ymin, xmax, ymax])\n","        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n","        \n","        labels = torch.tensor([self.label])\n","        \n","        image_id = torch.tensor([idx+1])\n","        # area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n","        # suppose all instances are not crowd\n","        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n","        \n","        target = {}\n","        target[\"boxes\"] = boxes\n","        target[\"labels\"] = labels\n","        target[\"image_id\"] = image_id\n","        \n","        transformed = self.transforms(image=img, bboxes=boxes, labels=labels)\n","       \n","        tfm_img = transformed['image']\n","        tfm_target={}\n","        tfm_target[\"boxes\"] = torch.as_tensor(transformed['bboxes']).squeeze(0)\n","        tfm_target[\"labels\"] = torch.as_tensor(transformed['labels']).squeeze(0)\n","        tfm_target[\"image_id\"] = image_id\n","        \n","        tfm_boxes = np.array(tfm_target[\"boxes\"].unsqueeze(0))\n","        tfm_area = (tfm_boxes[:, 3] - tfm_boxes[:, 1]) * (tfm_boxes[:, 2] - tfm_boxes[:, 0])\n","        tfm_target[\"area\"] = tfm_area\n","        tfm_target[\"iscrowd\"] = iscrowd\n","        \n","        targets=[]\n","        targets.append(tfm_target)\n","        \n","        return tfm_img, targets"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-17T09:18:30.77537Z","iopub.status.busy":"2022-04-17T09:18:30.77492Z","iopub.status.idle":"2022-04-17T09:18:30.792366Z","shell.execute_reply":"2022-04-17T09:18:30.791648Z","shell.execute_reply.started":"2022-04-17T09:18:30.775329Z"},"trusted":true},"outputs":[],"source":["class ChestXRayDatasetValid(Dataset):\n","\n","    def __init__(self, files, label, tfm=None):\n","        data_path = \"../input/chest-xray-box\"\n","        self.files = files\n","        self.transforms = tfm\n","        self.label = label\n","        if label == 1:\n","            data_path = data_path+\"/NORMAL\"\n","        else:\n","            data_path = data_path+\"/PNEUMONIA\"\n","        self.img_files = [os.path.join(data_path+\"/JPEGFiles\", x) for x in files]\n","        self.rectboxes = [os.path.join(data_path+\"/Rectbox\", x.replace(\".jpeg\",\".xml\")) for x in files]\n","    \n","    def __len__(self):\n","        return len(self.files)\n","  \n","    def __getitem__(self,idx):\n","        fname = self.img_files[idx]\n","        gray_img = cv2.imread(fname, cv2.IMREAD_GRAYSCALE)\n","        img = cv2.cvtColor(gray_img, cv2.COLOR_GRAY2BGR)\n","        img = img.astype(np.float32)\n","        img /= 255\n","        # im = torch.tensor(im)\n","        # img = self.transform(img)\n","        num_objs = 1\n","           \n","        xml = ET.parse(self.rectboxes[idx])\n","        root = xml.getroot()\n","        obj = root.find(\"object\")\n","        bndbox = obj.find(\"bndbox\")\n","        \n","        boxes = []\n","        xmin = int(bndbox[0].text)\n","        ymin = int(bndbox[1].text)\n","        xmax = int(bndbox[2].text)\n","        ymax = int(bndbox[3].text)\n","        boxes.append([xmin, ymin, xmax, ymax])\n","        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n","        \n","        labels = torch.tensor([self.label])\n","        \n","        image_id = torch.tensor(idx)\n","        # area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n","        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n","        \n","        target = {}\n","        target[\"boxes\"] = boxes\n","        target[\"labels\"] = labels\n","        target[\"image_id\"] = image_id\n","        \n","        transformed = self.transforms(image=img, bboxes=boxes, labels=labels)\n","    \n","        tfm_img = transformed['image']\n","        tfm_target={}\n","        tfm_target[\"boxes\"] = torch.as_tensor(transformed['bboxes'])\n","        tfm_target[\"labels\"] = torch.as_tensor(transformed['labels'])\n","        tfm_target[\"image_id\"] = image_id\n","        \n","        tfm_boxes = np.array(tfm_target[\"boxes\"])\n","        tfm_area = (tfm_boxes[:, 3] - tfm_boxes[:, 1]) * (tfm_boxes[:, 2] - tfm_boxes[:, 0])\n","        tfm_target[\"area\"] = tfm_area\n","        tfm_target[\"iscrowd\"] = iscrowd\n","        \n","        # targets=[]\n","        # targets.append(tfm_target)\n","        \n","        return tfm_img, tfm_target"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-17T09:18:30.796101Z","iopub.status.busy":"2022-04-17T09:18:30.795867Z","iopub.status.idle":"2022-04-17T09:18:30.808985Z","shell.execute_reply":"2022-04-17T09:18:30.808129Z","shell.execute_reply.started":"2022-04-17T09:18:30.796072Z"},"trusted":true},"outputs":[],"source":["class ChestXRayDatasetTest(Dataset):\n","\n","    def __init__(self, files, label, tfm=None):\n","        data_path = \"../input/chest-xray-box\"\n","        self.files = files\n","        self.transforms = tfm\n","        self.label = label\n","        if label == 1:\n","            data_path = data_path+\"/NORMAL\"\n","        else:\n","            data_path = data_path+\"/PNEUMONIA\"\n","        self.img_files = [os.path.join(data_path+\"/JPEGFiles\", x) for x in files]\n","        # self.rectboxes = [os.path.join(data_path+\"/Rectbox\", x.replace(\".jpeg\",\".xml\")) for x in files]\n","    \n","    def __len__(self):\n","        return len(self.files)\n","  \n","    def __getitem__(self,idx):\n","        fname = self.img_files[idx]\n","        gray_img = cv2.imread(fname, cv2.IMREAD_GRAYSCALE)\n","        img = cv2.cvtColor(gray_img, cv2.COLOR_GRAY2BGR)\n","        img = img.astype(np.float32)\n","        img /= 255\n","        # im = torch.tensor(im)\n","        # img = self.transform(img)\n","        transformed = self.transforms(image=img)\n","        \n","        return transformed['image'], self.label"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-17T09:18:30.810756Z","iopub.status.busy":"2022-04-17T09:18:30.81031Z","iopub.status.idle":"2022-04-17T09:18:30.829473Z","shell.execute_reply":"2022-04-17T09:18:30.828754Z","shell.execute_reply.started":"2022-04-17T09:18:30.810697Z"},"id":"4uFH05JAS--2","trusted":true},"outputs":[],"source":["# 0 for background\n","n_train_set = ChestXRayDatasetTrain(n_train_files, label=1, tfm=train_tfm)\n","n_valid_set = ChestXRayDatasetValid(n_valid_files, label=1, tfm=valid_tfm)\n","n_test_set = ChestXRayDatasetTest(n_test_files, label=1, tfm=test_tfm)\n","\n","p_train_set = ChestXRayDatasetTrain(p_train_files, label=2, tfm=train_tfm)\n","p_valid_set = ChestXRayDatasetValid(p_valid_files, label=2, tfm=valid_tfm)\n","p_test_set = ChestXRayDatasetTest(p_test_files, label=2, tfm=test_tfm)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-17T09:18:30.833108Z","iopub.status.busy":"2022-04-17T09:18:30.832039Z","iopub.status.idle":"2022-04-17T09:18:30.838848Z","shell.execute_reply":"2022-04-17T09:18:30.838138Z","shell.execute_reply.started":"2022-04-17T09:18:30.833068Z"},"id":"lpKQWQzYS--4","trusted":true},"outputs":[],"source":["train_set = n_train_set + p_train_set\n","valid_set = n_valid_set + p_valid_set\n","test_set = n_test_set + p_test_set"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-17T09:18:30.841771Z","iopub.status.busy":"2022-04-17T09:18:30.839739Z","iopub.status.idle":"2022-04-17T09:18:30.847568Z","shell.execute_reply":"2022-04-17T09:18:30.846892Z","shell.execute_reply.started":"2022-04-17T09:18:30.84173Z"},"id":"2cSkRkT4S--4","outputId":"ca41a6df-7e43-42c0-e740-b4a9adaf02df","trusted":true},"outputs":[],"source":["print(len(train_set), len(valid_set), len(test_set))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-17T09:18:30.84963Z","iopub.status.busy":"2022-04-17T09:18:30.849068Z","iopub.status.idle":"2022-04-17T09:18:30.855413Z","shell.execute_reply":"2022-04-17T09:18:30.854695Z","shell.execute_reply.started":"2022-04-17T09:18:30.849593Z"},"id":"Tfmn9aknS--5","trusted":true},"outputs":[],"source":["batch_size = 1"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-17T09:18:30.857753Z","iopub.status.busy":"2022-04-17T09:18:30.857524Z","iopub.status.idle":"2022-04-17T09:18:30.864284Z","shell.execute_reply":"2022-04-17T09:18:30.863524Z","shell.execute_reply.started":"2022-04-17T09:18:30.857717Z"},"id":"3ysyW4AbS--5","trusted":true},"outputs":[],"source":["train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n","valid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["BOX_COLOR = (255, 0, 0) # Red\n","TEXT_COLOR = (255, 255, 255) # White\n","\n","\n","def visualize_bbox(img, bbox, class_name, color=BOX_COLOR, thickness=2):\n","    \"\"\"Visualizes a single bounding box on the image\"\"\"\n","    x_min, y_min, x_max, y_max = bbox\n","    x_min, y_min, x_max, y_max = int(x_min), int(y_min), int(x_max), int(y_max)\n","   \n","    cv2.rectangle(img, (x_min, y_min), (x_max, y_max), color=color, thickness=thickness)\n","    \n","    ((text_width, text_height), _) = cv2.getTextSize(class_name, cv2.FONT_HERSHEY_SIMPLEX, 0.35, 1)    \n","    cv2.rectangle(img, (x_min, y_min - int(1.3 * text_height)), (x_min + text_width, y_min), BOX_COLOR, -1)\n","    cv2.putText(\n","        img,\n","        text=class_name,\n","        org=(x_min, y_min - int(0.3 * text_height)),\n","        fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n","        fontScale=0.35, \n","        color=TEXT_COLOR, \n","        lineType=cv2.LINE_AA,\n","    )\n","    return img\n","\n","\n","def visualize(image, bboxes, category_ids, category_id_to_name):\n","    img = image.copy()\n","    for bbox, category_id in zip(bboxes, category_ids):\n","        class_name = category_id_to_name[category_id]\n","        img = visualize_bbox(img, bbox, class_name)\n","    plt.figure(figsize=(12, 12))\n","    # plt.axis('off')\n","    plt.imshow(img)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def draw_train_img(data):\n","    label_to_name  = {1: 'NORMAL', 2: 'PNEUMONIA'}\n","    img = np.array(data[0].permute(1,2,0)) # tensor CHW -> numpy HWC\n","    target = data[1][0] # only one target\n","    bboxes = []\n","    bboxes.append(target[\"boxes\"])\n","    labels = []\n","    labels.append(int(target[\"labels\"]))\n","    visualize(\n","    img,\n","    bboxes,\n","    labels,\n","    label_to_name)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-17T09:18:30.865958Z","iopub.status.busy":"2022-04-17T09:18:30.865632Z","iopub.status.idle":"2022-04-17T09:18:30.907729Z","shell.execute_reply":"2022-04-17T09:18:30.907039Z","shell.execute_reply.started":"2022-04-17T09:18:30.865925Z"},"trusted":true},"outputs":[],"source":["train_set[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-17T09:18:30.90934Z","iopub.status.busy":"2022-04-17T09:18:30.909094Z","iopub.status.idle":"2022-04-17T09:18:30.94368Z","shell.execute_reply":"2022-04-17T09:18:30.942858Z","shell.execute_reply.started":"2022-04-17T09:18:30.909308Z"},"trusted":true},"outputs":[],"source":["train_set[0][1]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-17T09:18:30.945758Z","iopub.status.busy":"2022-04-17T09:18:30.945233Z","iopub.status.idle":"2022-04-17T09:18:31.422573Z","shell.execute_reply":"2022-04-17T09:18:31.421945Z","shell.execute_reply.started":"2022-04-17T09:18:30.945692Z"},"trusted":true},"outputs":[],"source":["draw_train_img(train_set[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-17T09:18:31.424231Z","iopub.status.busy":"2022-04-17T09:18:31.423892Z","iopub.status.idle":"2022-04-17T09:18:31.433896Z","shell.execute_reply":"2022-04-17T09:18:31.433104Z","shell.execute_reply.started":"2022-04-17T09:18:31.4242Z"},"id":"ZSOE6IUHX3GP","outputId":"35518e88-f2fd-4612-f5be-7f42921c3f2e","trusted":true},"outputs":[],"source":["# \"cuda\" only when GPUs are available.\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","device"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-17T09:18:31.446255Z","iopub.status.busy":"2022-04-17T09:18:31.446067Z","iopub.status.idle":"2022-04-17T09:18:32.145395Z","shell.execute_reply":"2022-04-17T09:18:32.144722Z","shell.execute_reply.started":"2022-04-17T09:18:31.446232Z"},"trusted":true},"outputs":[],"source":["from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n","# Initialize a model, and put it on the device specified.\n","\n","import torchvision.models as models\n","# model = models.detection.fasterrcnn_mobilenet_v3_large_fpn(pretrained=True)\n","model = models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n","\n","num_classes = 3  # 2 class + background\n","# get number of input features for the classifier\n","in_features = model.roi_heads.box_predictor.cls_score.in_features\n","# replace the pre-trained head with a new one\n","model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n","\n","model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-17T09:18:32.147341Z","iopub.status.busy":"2022-04-17T09:18:32.146801Z","iopub.status.idle":"2022-04-17T09:33:15.18851Z","shell.execute_reply":"2022-04-17T09:33:15.187761Z","shell.execute_reply.started":"2022-04-17T09:18:32.1473Z"},"trusted":true},"outputs":[],"source":["# from engine import train_one_epoch, evaluate\n","\n","# construct an optimizer\n","params = [p for p in model.parameters() if p.requires_grad]\n","optimizer = torch.optim.SGD(params, lr=0.005,\n","                        momentum=0.9, weight_decay=0.0005)\n","# and a learning rate scheduler\n","lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n","                                            step_size=3,\n","                                            gamma=0.1)\n","\n","num_epochs = 30\n","\n","for epoch in range(num_epochs):\n","\n","    train_one_epoch(model, optimizer, train_loader, device, epoch, print_freq=10)\n","    # update the learning rate\n","    lr_scheduler.step()\n","    # evaluate on the test dataset\n","    evaluate(model, valid_loader, device=device)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-17T09:33:15.294258Z","iopub.status.busy":"2022-04-17T09:33:15.294006Z","iopub.status.idle":"2022-04-17T09:33:15.303503Z","shell.execute_reply":"2022-04-17T09:33:15.302756Z","shell.execute_reply.started":"2022-04-17T09:33:15.294225Z"},"id":"FPrhfPQKLIPq","trusted":true},"outputs":[],"source":["test_loader = DataLoader(test_set, batch_size=1, shuffle=False, num_workers=0, pin_memory=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-17T09:33:15.305236Z","iopub.status.busy":"2022-04-17T09:33:15.304972Z","iopub.status.idle":"2022-04-17T09:40:07.55534Z","shell.execute_reply":"2022-04-17T09:40:07.554667Z","shell.execute_reply.started":"2022-04-17T09:33:15.305201Z"},"trusted":true},"outputs":[],"source":["confusion_mat = np.zeros((2, 2))\n","for batch in tqdm(test_loader):\n","    imgs, labels = batch\n","    model.eval()\n","    predictions = model(imgs.to(device))          # Returns predictions\n","    \n","    for pred, truth in zip(predictions, labels):\n","        # print(\"One batch\")\n","        # print(pred, truth)\n","        test_label = int(pred[\"labels\"][0])\n","        if (test_label == 2):\n","            if(test_label == int(truth)):\n","                confusion_mat[0][0] = confusion_mat[0][0] + 1\n","            else:\n","                confusion_mat[0][1] = confusion_mat[0][1] + 1\n","        elif (test_label == 1): \n","            if(test_label == int(truth)):\n","                confusion_mat[1][1] = confusion_mat[1][1] + 1\n","            else:\n","                confusion_mat[1][0] = confusion_mat[1][0] + 1"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-17T09:40:07.557379Z","iopub.status.busy":"2022-04-17T09:40:07.556605Z","iopub.status.idle":"2022-04-17T09:40:07.568836Z","shell.execute_reply":"2022-04-17T09:40:07.567715Z","shell.execute_reply.started":"2022-04-17T09:40:07.55734Z"},"trusted":true},"outputs":[],"source":["total = confusion_mat[0][0] +  confusion_mat[1][0] +  confusion_mat[0][1] + confusion_mat[1][1]\n","print(\"confusion_mat\", confusion_mat)\n","print(\"accuracy\", (confusion_mat[0][0] + confusion_mat[1][1]) / total)\n","precision =  confusion_mat[0][0] / (confusion_mat[0][0] + confusion_mat[0][1])\n","recall = confusion_mat[0][0] / (confusion_mat[0][0] + confusion_mat[1][0])\n","print(\"precision\", precision)\n","print(\"recall\", recall)\n","print(\"f1\", 2*precision*recall / (precision+recall))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-17T09:40:07.570633Z","iopub.status.busy":"2022-04-17T09:40:07.570339Z","iopub.status.idle":"2022-04-17T09:40:07.579439Z","shell.execute_reply":"2022-04-17T09:40:07.578662Z","shell.execute_reply.started":"2022-04-17T09:40:07.570594Z"},"trusted":true},"outputs":[],"source":["def visualize_test(image, bboxes, category_ids, category_id_to_name):\n","    img = image.copy()\n","    for bbox, category_id in zip(bboxes, category_ids):\n","        class_name = category_id_to_name[category_id]\n","        img = visualize_bbox(img, bbox, class_name)\n","    # plt.figure(figsize=(12, 12))\n","    # plt.axis('off')\n","    plt.imshow(img)\n","\n","def draw_test_img(data):\n","    label_to_name  = {1: 'NORMAL', 2: 'PNEUMONIA'}\n","    img = np.array(data[0].permute(1,2,0)) # tensor CHW -> numpy HWC\n","    target = data[1][0] # only one target\n","    bboxes = []\n","    bboxes.append(target[\"boxes\"])\n","    labels = []\n","    labels.append(int(target[\"labels\"]))\n","    visualize_test(\n","    img,\n","    bboxes,\n","    labels,\n","    label_to_name)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-17T09:49:05.506101Z","iopub.status.busy":"2022-04-17T09:49:05.505761Z","iopub.status.idle":"2022-04-17T09:49:07.345598Z","shell.execute_reply":"2022-04-17T09:49:07.34453Z","shell.execute_reply.started":"2022-04-17T09:49:05.506064Z"},"trusted":true},"outputs":[],"source":["plt.figure(figsize=(16, 8))\n","plt.suptitle('Inference PNEUMONIA')\n","label_to_name  = {1: 'NORMAL', 2: 'PNEUMONIA'}\n","for i in range(8):\n","        img = p_test_set[i][0]\n","        model.eval()\n","        predictions = model(img.unsqueeze(0).to(device))          # Returns predictions\n","\n","        targets = []\n","        target = {}\n","        target[\"boxes\"] = predictions[0][\"boxes\"][0]\n","        target[\"labels\"] = int(predictions[0][\"labels\"][0])\n","        targets.append(target)\n","        plt.subplot(2, 4, i + 1).set_title(f'pred: {label_to_name[target[\"labels\"]]}')\n","        \n","        data = [img, targets]\n","        draw_test_img(data)\n","\n","plt.tight_layout()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-17T09:49:34.693749Z","iopub.status.busy":"2022-04-17T09:49:34.693464Z","iopub.status.idle":"2022-04-17T09:49:36.647613Z","shell.execute_reply":"2022-04-17T09:49:36.64698Z","shell.execute_reply.started":"2022-04-17T09:49:34.6937Z"},"trusted":true},"outputs":[],"source":["plt.figure(figsize=(16, 8))\n","plt.suptitle('Inference NORMAL')\n","label_to_name  = {1: 'NORMAL', 2: 'PNEUMONIA'}\n","for i in range(8):\n","        img = n_test_set[i][0]\n","        model.eval()\n","        predictions = model(img.unsqueeze(0).to(device))          # Returns predictions\n","\n","        targets = []\n","        target = {}\n","        target[\"boxes\"] = predictions[0][\"boxes\"][0]\n","        target[\"labels\"] = int(predictions[0][\"labels\"][0])\n","        targets.append(target)\n","        plt.subplot(2, 4, i + 1).set_title(f'pred: {label_to_name[target[\"labels\"]]}')\n","        \n","        data = [img, targets]\n","        draw_test_img(data)\n","\n","plt.tight_layout()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
